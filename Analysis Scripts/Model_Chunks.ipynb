{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5742c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/rutvikdhopate/Downloads/Jupyter_Files/Clinical-Trials-Eligibility/\"))\n",
    "from PyTorch_Files.encoder_model import SelfAttention, MultiHeadSelfAttention, FeedForwardNetwork, Encoder, Transformer, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdba06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (W_q): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (W_k): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (W_v): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (W_o): Linear(in_features=8, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sa = SelfAttention(d_model=128, d_k=8, d_v=8)\n",
    "print(sa)\n",
    "mhsa = MultiHeadSelfAttention(d_model=128, d_k=8, d_v=8, n_heads=4)\n",
    "ffn = FeedForwardNetwork(d_model=128, d_ff=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24491e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size torch.Size([4, 8, 128])\n",
      "Self Attention Size: torch.Size([4, 8, 128])\n",
      "Multi Head Self Attention Size: torch.Size([4, 8, 128])\n",
      "MHSA Output 1st example in batch: tensor([[ 0.0080, -0.0329,  0.1310,  ..., -0.1632, -0.0729, -0.2994],\n",
      "        [-0.0079,  0.0272,  0.1288,  ..., -0.1005, -0.0491, -0.2562],\n",
      "        [ 0.0012, -0.0258,  0.0709,  ..., -0.1141, -0.0418, -0.2869],\n",
      "        ...,\n",
      "        [ 0.0389, -0.0528,  0.0900,  ..., -0.1330, -0.0439, -0.2535],\n",
      "        [-0.0131,  0.0079,  0.0973,  ..., -0.1163, -0.0261, -0.2475],\n",
      "        [-0.0495, -0.0214,  0.1042,  ..., -0.0770, -0.0284, -0.2744]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "FFN Output: tensor([[-0.0119, -0.0200, -0.0801,  ..., -0.0386, -0.0356, -0.0175],\n",
      "        [-0.0169, -0.0069, -0.0701,  ..., -0.0302, -0.0394, -0.0157],\n",
      "        [-0.0084, -0.0069, -0.0892,  ..., -0.0476, -0.0333, -0.0169],\n",
      "        ...,\n",
      "        [-0.0127, -0.0153, -0.0802,  ..., -0.0328, -0.0356, -0.0089],\n",
      "        [-0.0114, -0.0102, -0.0823,  ..., -0.0357, -0.0305, -0.0208],\n",
      "        [-0.0124, -0.0162, -0.0716,  ..., -0.0466, -0.0339, -0.0177]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Shape after FFN: torch.Size([4, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "batch = [\n",
    "    {'input_ids': torch.tensor([1, 15, 1075, 126, 194, 430, 105, 873]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 34, 105, 65, 14, 43, 5, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 0, 0]), 'label': torch.tensor(0)},\n",
    "    {'input_ids': torch.tensor([1, 1500, 1075, 126, 194, 430, 15, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 0, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 1050, 755, 16, 20, 4, 105, 83]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 0]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "# Stack tensors into batch form\n",
    "input_ids = torch.stack([b['input_ids'] for b in batch])         # [4, 8]\n",
    "attention_mask = torch.stack([b['attention_mask'] for b in batch]) # [4, 8]\n",
    "labels = torch.stack([b['label'] for b in batch])\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=16000, embedding_dim=128)\n",
    "embedded_data = embedding(input_ids)  # [4, 8, 128]\n",
    "\n",
    "x = embedded_data\n",
    "# Each batch has dimensions [batch_size, seq_len, embedding_dim]\n",
    "print(f\"Batch Size {embedded_data.size()}\")\n",
    "\n",
    "# On applying Self Attention for each batch, the size becomes [batch_size, seq_len, d_v]\n",
    "print(f\"Self Attention Size: {sa(embedded_data).size()}\")\n",
    "\n",
    "# Multi Head Self Attention Size -> [batch_size, seq_len, n_heads*d_v] because the outputs from each attention head concatenated along last dim\n",
    "print(f\"Multi Head Self Attention Size: {mhsa(embedded_data).size()}\")\n",
    "x = mhsa(x)\n",
    "print(f\"MHSA Output 1st example in batch: {x[0,:,:]}\")\n",
    "\n",
    "# FFN -> \n",
    "x = ffn(x)\n",
    "print(f\"FFN Output: {x[0,:,:]}\")\n",
    "print(f\"Shape after FFN: {x.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42bbca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for the first example in the batch after one encoder layer:\n",
      " tensor([[-0.6904,  0.2927,  0.1687,  ...,  1.3598,  0.3392,  0.2428],\n",
      "        [ 1.0353,  0.1211,  0.2290,  ..., -0.3489, -0.2762,  1.8946],\n",
      "        [-0.4611,  1.4712, -0.4305,  ..., -0.3563,  0.0455, -1.4414],\n",
      "        ...,\n",
      "        [ 1.0105,  0.9443,  0.1113,  ...,  0.5987,  0.1199,  0.1417],\n",
      "        [-0.3019, -0.5907, -0.7737,  ...,  0.9657,  0.2028,  0.1290],\n",
      "        [-0.1351, -2.6033,  1.6679,  ..., -0.5404, -0.1157, -0.0051]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([4, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(d_model=128, d_k=16, d_v=16, n_heads=8, d_ff=512)\n",
    "\n",
    "batch = [\n",
    "    {'input_ids': torch.tensor([1, 15, 1075, 126, 194, 430, 105, 873]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 34, 105, 65, 14, 43, 5, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 0, 0]), 'label': torch.tensor(0)},\n",
    "    {'input_ids': torch.tensor([1, 1500, 1075, 126, 194, 430, 15, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 0, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 1050, 755, 16, 20, 4, 105, 83]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 0]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "# Stack tensors into batch form\n",
    "input_ids = torch.stack([b['input_ids'] for b in batch])         # [4, 8]\n",
    "attention_mask = torch.stack([b['attention_mask'] for b in batch]) # [4, 8]\n",
    "labels = torch.stack([b['label'] for b in batch])\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=16000, embedding_dim=128)\n",
    "embedded_data = embedding(input_ids)  # [4, 8, 128]\n",
    "\n",
    "x = embedded_data\n",
    "print(f\"Output for the first example in the batch after one encoder layer:\\n {enc(x[0,:,:])}\")\n",
    "print(enc(x).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ef2280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for the first example in the batch after n transformer layers:\n",
      "tensor([[ 0.0622, -0.1400, -1.0187,  ..., -0.3090,  1.7392,  0.8000],\n",
      "        [ 0.0126, -1.6979, -1.4032,  ...,  0.6270,  0.5518,  2.2303],\n",
      "        [-0.7923,  0.8259,  0.2630,  ..., -0.1014, -0.5340, -0.8316],\n",
      "        ...,\n",
      "        [-0.1460, -1.4061,  0.4073,  ...,  0.1582, -0.5481, -1.2606],\n",
      "        [-0.9957,  1.7721,  0.4970,  ..., -0.1635,  0.6927,  1.1278],\n",
      "        [-1.2004,  0.4545, -2.3351,  ...,  0.0312,  0.1477,  1.4403]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([4, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "trans = Transformer(vocab_size=16000, d_model=128, d_k=16, d_v=16, n_heads=8, d_ff=512, n_layers=4)\n",
    "\n",
    "batch = [\n",
    "    {'input_ids': torch.tensor([1, 15, 1075, 126, 194, 430, 105, 873]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 34, 105, 65, 14, 43, 5, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 0, 0]), 'label': torch.tensor(0)},\n",
    "    {'input_ids': torch.tensor([1, 1500, 1075, 126, 194, 430, 15, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 0, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 1050, 755, 16, 20, 4, 105, 83]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 0]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "# Stack tensors into batch form\n",
    "input_ids = torch.stack([b['input_ids'] for b in batch])         # [4, 8]\n",
    "attention_mask = torch.stack([b['attention_mask'] for b in batch]) # [4, 8]\n",
    "labels = torch.stack([b['label'] for b in batch])\n",
    "\n",
    "\"\"\"\n",
    "Each batch goes through all the layers of the transformer architecture.\n",
    "At each encoder layer, x goes through MHSA -> then residual + layer norm -> then FFN -> then again residual + layer norm\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Output for the first example in the batch after n transformer layers:\\n{trans(input_ids)[0,:,:]}\")\n",
    "print(trans(input_ids).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aa277e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the output after the classification head\n",
      "tensor([[-0.1642,  0.0261, -0.3276],\n",
      "        [-0.1357, -0.2390, -0.4923],\n",
      "        [-0.3571,  0.1022, -0.3079],\n",
      "        [-0.0125, -0.1384, -0.3777]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clsfr = Classifier(vocab_size=16000, max_len=1024, d_model=128, d_k=16, d_v=16, n_heads=8, d_ff=512, n_layers=5, n_classes=3)\n",
    "\n",
    "batch = [\n",
    "    {'input_ids': torch.tensor([1, 15, 1075, 126, 194, 430, 105, 873]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 34, 105, 65, 14, 43, 5, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 0, 0]), 'label': torch.tensor(0)},\n",
    "    {'input_ids': torch.tensor([1, 1500, 1075, 126, 194, 430, 15, 3]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 0, 1, 1]), 'label': torch.tensor(1)},\n",
    "    {'input_ids': torch.tensor([1, 1050, 755, 16, 20, 4, 105, 83]), \n",
    "     'attention_mask': torch.tensor([1, 1, 1, 1, 1, 1, 1, 0]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "# Stack tensors into batch form\n",
    "input_ids = torch.stack([b['input_ids'] for b in batch])         # [4, 8]\n",
    "attention_mask = torch.stack([b['attention_mask'] for b in batch]) # [4, 8]\n",
    "labels = torch.stack([b['label'] for b in batch])\n",
    "\n",
    "print(\"This is the output after the classification head\")\n",
    "\"\"\"\n",
    "The classification head takes the output from the transformer and applies positional encoding and computes the mean along the first dimension\n",
    "\"\"\"\n",
    "out = clsfr(input_ids)\n",
    "print(clsfr(input_ids, attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3773a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=clsfr.parameters(), lr=1e-4)\n",
    "\n",
    "loss = loss_fn(out, labels)\n",
    "pred = out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "796d106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0062, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a86ce20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9730daa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c794e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0062, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
