{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer \n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import wandb\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- Configuration ---\n",
    "config_file_path = '/Users/nishitha/Desktop/Learn/NLP/Clinical trials eligibility/config.toml'\n",
    "if not os.path.exists(config_file_path):\n",
    "    print(f\"Error: Config file not found at {config_file_path}\")\n",
    "else:\n",
    "    config = toml.load(config_file_path)\n",
    "\n",
    "    # general parameters\n",
    "    BATCH_SIZE = config['training']['batch_size']\n",
    "    NUM_LABELS = config['data']['num_labels']\n",
    "    LEARNING_RATE = config['training']['learning_rate']\n",
    "\n",
    "    # For BioBert Transformer \n",
    "    EPOCHS_TRANSFORMER = config['training']['transformer']['epochs']\n",
    "    MAX_LEN_TRANSFORMER = config['training']['transformer']['max_len']\n",
    "\n",
    "    # For RNN model parameters\n",
    "    MAX_LEN_RNN = config['data']['max_len']\n",
    "    EPOCHS_RNN = config['training']['rnn']['epochs']\n",
    "    EMBEDDING_DIM = config['model']['embedding_dim']\n",
    "    RNN_HIDDEN_SIZE = config['model']['rnn']['rnn_hidden_size'] \n",
    "    RNN_NUM_LAYERS = config['model']['rnn']['rnn_num_layers']   \n",
    "    RNN_DROPOUT = config['model']['rnn']['rnn_dropout']        \n",
    "\n",
    "    # device configuration\n",
    "    DEVICE = torch.device(config['general']['device'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCreator_RNN(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.patient_texts = list(df['patient'])\n",
    "        self.criteria_texts = list(df['criteria'])\n",
    "        self.labels = list(df['label'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        if self.pad_token_id is None:\n",
    "            print(\"Warning: [PAD] token not found in tokenizer. Using 0 for padding token ID in DataCreator.\")\n",
    "            self.pad_token_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patient_text = str(self.patient_texts[index])\n",
    "        criteria_text = str(self.criteria_texts[index])\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer.encode(criteria_text, patient_text)\n",
    "\n",
    "        ids = encoding.ids\n",
    "        attention_mask = encoding.attention_mask\n",
    "\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[:self.max_len]\n",
    "            attention_mask = attention_mask[:self.max_len]\n",
    "        else:\n",
    "            padding_length = self.max_len - len(ids)\n",
    "            ids += [self.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"label\": label,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCreator_Transformer(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.patient_texts = list(df['patient'])\n",
    "        self.criteria_texts = list(df['criteria'])\n",
    "        self.labels = list(df['label'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            print(\"Warning: [PAD] token not found in tokenizer. Using 0 for padding token ID.\")\n",
    "            self.pad_token_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patient_text = str(self.patient_texts[index])\n",
    "        criteria_text = str(self.criteria_texts[index])\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            criteria_text,\n",
    "            patient_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifierFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int, num_layers: int, num_labels: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.dropout_classifier = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids) \n",
    "\n",
    "        lengths = attention_mask.sum(dim=1)\n",
    "        \n",
    "        lengths = lengths.cpu().clamp(min=1) \n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False \n",
    "        )\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        final_hidden_state = hidden[-1, :, :]\n",
    "        \n",
    "        pooled_output = self.dropout_classifier(final_hidden_state)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_transformer_model(\n",
    "    model_name: str,\n",
    "    df: pd.DataFrame,\n",
    "    num_labels: int,\n",
    "    max_len: int,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: torch.device\n",
    "):\n",
    "    print(f\"\\n--- Starting training for Transformer model: {model_name} ---\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Tokenizer for {model_name} loaded.\")\n",
    "\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    df.dropna(subset=['label'], inplace=True)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=0.25, stratify=train_val_df['label'], random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split: Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n",
    "\n",
    "    train_dataset = DataCreator_Transformer(df=train_df, tokenizer=tokenizer, max_len=max_len)\n",
    "    val_dataset = DataCreator_Transformer(df=val_df, tokenizer=tokenizer, max_len=max_len)\n",
    "    test_dataset = DataCreator_Transformer(df=test_df, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TransformerClassifier(model_name, num_labels)\n",
    "    model.to(device)\n",
    "    print(f\"Model {model_name} initialized\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    model_save_name = model_name.replace('/', '_').replace('-', '_')\n",
    "    model_save_path = f\"{model_save_name}_clinical_model.pt\"\n",
    "\n",
    "    \n",
    "    wandb.init(project='NLP_Project_Clinical_Trials', config=config)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} (Model: {model_name})\")\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_val_preds += (preds == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = correct_val_preds / len(val_dataset)*100\n",
    "\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        wandb.log(data={\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path} (Best validation accuracy: {best_val_accuracy:.4f})\")\n",
    "    \n",
    "    print(f\"\\n--- Finished training for {model_name} ---\")\n",
    "\n",
    "    print(f\"\\n--- Evaluating {model_name} on the TEST SET ---\")\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    correct_test_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Test Evaluation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_test_preds += (preds == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_test_preds / len(test_dataset)\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"final_val_loss\": avg_val_loss,\n",
    "        \"final_val_accuracy\": val_accuracy,\n",
    "        \"best_val_accuracy\": best_val_accuracy,\n",
    "        \"final_test_loss\": avg_test_loss,\n",
    "        \"final_test_accuracy\": test_accuracy,\n",
    "        \"saved_model_path\": model_save_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_rnn_model(\n",
    "    df: pd.DataFrame,\n",
    "    num_labels: int,\n",
    "    max_len: int,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    learning_rate: float,\n",
    "    embedding_dim: int,\n",
    "    hidden_size: int,\n",
    "    num_rnn_layers: int,\n",
    "    dropout: float,\n",
    "    device: torch.device\n",
    "):\n",
    "    print(f\"\\n--- Starting training for Simple RNN Model ---\")\n",
    "\n",
    "    tokenizer = Tokenizer.from_file(\"/Users/nishitha/Desktop/Learn/NLP/Clinical trials eligibility/BPE/bpe_tokenizer.json\")\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f\"Custom tokenizer loaded. Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    df.dropna(subset=['label'], inplace=True)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=0.25, stratify=train_val_df['label'], random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split: Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n",
    "\n",
    "    train_dataset = DataCreator_RNN(df=train_df, tokenizer=tokenizer, max_len=max_len)\n",
    "    val_dataset = DataCreator_RNN(df=val_df, tokenizer=tokenizer, max_len=max_len)\n",
    "    test_dataset = DataCreator_RNN(df=test_df, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = RNNClassifierFromScratch(vocab_size, embedding_dim, hidden_size, num_rnn_layers, num_labels, dropout)\n",
    "    model.to(device)\n",
    "    print(f\"RNNClassifierFromScratch initialized on {device}.\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    model_save_path = \"scratch_rnn_clinical_model.pt\"\n",
    "\n",
    "    wandb.init(project='NLP_Project_Clinical_Trials', config=config)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} (Model: Simple RNN)\")\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_val_preds += (preds == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = correct_val_preds / len(val_dataset)*100\n",
    "\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        wandb.log(data={\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path} (Best validation accuracy: {best_val_accuracy:.4f})\")\n",
    "    \n",
    "    print(f\"\\n--- Finished training for Simple RNN Model ---\")\n",
    "\n",
    "    print(f\"\\n--- Evaluating Simple RNN Model on the TEST SET ---\")\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    correct_test_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Test Evaluation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_test_preds += (preds == labels).sum().item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    test_accuracy = correct_test_preds / len(test_dataset)\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"model_name\": \"Simple RNN Classifier\",\n",
    "        \"final_val_loss\": avg_val_loss,\n",
    "        \"final_val_accuracy\": val_accuracy,\n",
    "        \"best_val_accuracy\": best_val_accuracy,\n",
    "        \"final_test_loss\": avg_test_loss,\n",
    "        \"final_test_accuracy\": test_accuracy,\n",
    "        \"saved_model_path\": model_save_path\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting training for Transformer model: dmis-lab/biobert-v1.1 ---\n",
      "Tokenizer for dmis-lab/biobert-v1.1 loaded.\n",
      "Dataset split: Train=619 | Val=207 | Test=207\n",
      "Model dmis-lab/biobert-v1.1 initialized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-frog-20</strong> at: <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ii2htsql' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ii2htsql</a><br> View project at: <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250803_193022-ii2htsql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nishitha/Desktop/Learn/NLP/Clinical trials eligibility/PyTorch_Files/wandb/run-20250803_193231-ng4nzcv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ng4nzcv4' target=\"_blank\">divine-water-21</a></strong> to <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ng4nzcv4' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ng4nzcv4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1 (Model: dmis-lab/biobert-v1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]/Users/nishitha/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1:   5%|▌         | 2/39 [04:13<1:06:39, 108.09s/it, loss=nan] wandb-core(73149) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73158) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:   8%|▊         | 3/39 [04:48<44:41, 74.48s/it, loss=nan]   wandb-core(73170) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73183) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73198) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  10%|█         | 4/39 [05:42<38:49, 66.54s/it, loss=nan]wandb-core(73215) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73237) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73241) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73255) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  13%|█▎        | 5/39 [06:41<36:07, 63.74s/it, loss=nan]wandb-core(73279) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73288) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73299) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  15%|█▌        | 6/39 [07:21<30:33, 55.55s/it, loss=nan]wandb-core(73306) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73313) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73314) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73325) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  18%|█▊        | 7/39 [08:23<30:48, 57.78s/it, loss=nan]wandb-core(73329) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73339) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73346) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  21%|██        | 8/39 [09:19<29:37, 57.35s/it, loss=nan]wandb-core(73378) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73385) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73403) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73412) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  23%|██▎       | 9/39 [10:15<28:22, 56.75s/it, loss=nan]wandb-core(73416) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73421) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73422) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73425) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  26%|██▌       | 10/39 [11:19<28:33, 59.09s/it, loss=nan]wandb-core(73436) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73448) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73454) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  28%|██▊       | 11/39 [12:07<26:00, 55.74s/it, loss=nan]wandb-core(73468) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73473) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73478) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  31%|███       | 12/39 [12:50<23:17, 51.74s/it, loss=nan]wandb-core(73494) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73500) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73506) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  33%|███▎      | 13/39 [13:35<21:31, 49.67s/it, loss=nan]wandb-core(73533) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73538) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73546) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  36%|███▌      | 14/39 [14:21<20:16, 48.67s/it, loss=nan]wandb-core(73554) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73563) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73565) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  38%|███▊      | 15/39 [15:10<19:29, 48.72s/it, loss=nan]wandb-core(73570) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73576) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  41%|████      | 16/39 [15:43<16:51, 43.97s/it, loss=nan]wandb-core(73581) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73600) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73609) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  44%|████▎     | 17/39 [16:22<15:37, 42.59s/it, loss=nan]wandb-core(73619) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73621) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  46%|████▌     | 18/39 [16:54<13:47, 39.40s/it, loss=nan]wandb-core(73626) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73659) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73667) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  49%|████▊     | 19/39 [17:41<13:49, 41.49s/it, loss=nan]wandb-core(73680) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73692) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73715) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73724) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  51%|█████▏    | 20/39 [18:32<14:07, 44.59s/it, loss=nan]wandb-core(73731) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73739) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73759) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  54%|█████▍    | 21/39 [19:17<13:23, 44.64s/it, loss=nan]wandb-core(73762) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73773) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  56%|█████▋    | 22/39 [19:53<11:54, 42.01s/it, loss=nan]wandb-core(73777) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73793) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73809) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  59%|█████▉    | 23/39 [20:40<11:36, 43.53s/it, loss=nan]wandb-core(73823) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73835) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73849) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73862) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  62%|██████▏   | 24/39 [21:32<11:31, 46.13s/it, loss=nan]wandb-core(73874) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73891) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73921) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  64%|██████▍   | 25/39 [22:21<10:55, 46.83s/it, loss=nan]wandb-core(73962) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73970) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  64%|██████▍   | 25/39 [22:59<10:55, 46.83s/it, loss=nan]wandb-core(73973) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  67%|██████▋   | 26/39 [22:59<09:36, 44.31s/it, loss=nan]wandb-core(73978) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(73992) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  69%|██████▉   | 27/39 [23:46<09:02, 45.17s/it, loss=nan]wandb-core(74014) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74039) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74054) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74057) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74066) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  72%|███████▏  | 28/39 [41:44<1:05:03, 354.90s/it, loss=nan]wandb-core(74072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74078) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74081) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  74%|███████▍  | 29/39 [42:26<43:30, 261.09s/it, loss=nan]  wandb-core(74083) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74087) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74098) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  77%|███████▋  | 30/39 [43:13<29:31, 196.84s/it, loss=nan]wandb-core(74103) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74107) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74116) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74123) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  79%|███████▉  | 31/39 [44:05<20:25, 153.20s/it, loss=nan]wandb-core(74127) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74144) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74154) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  82%|████████▏ | 32/39 [44:49<14:05, 120.73s/it, loss=nan]wandb-core(74161) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74163) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  85%|████████▍ | 33/39 [45:27<09:34, 95.81s/it, loss=nan] wandb-core(74167) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74176) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  87%|████████▋ | 34/39 [46:05<06:32, 78.57s/it, loss=nan]wandb-core(74182) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74195) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74203) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74211) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  90%|████████▉ | 35/39 [47:05<04:51, 72.81s/it, loss=nan]wandb-core(74215) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74226) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  92%|█████████▏| 36/39 [47:48<03:11, 63.89s/it, loss=nan]wandb-core(74245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74255) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74269) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  95%|█████████▍| 37/39 [48:32<01:55, 57.85s/it, loss=nan]wandb-core(74274) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74288) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74294) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  97%|█████████▋| 38/39 [49:13<00:52, 52.88s/it, loss=nan]wandb-core(74300) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74317) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1: 100%|██████████| 39/39 [50:16<00:00, 77.35s/it, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb-core(74354) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74360) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74367) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74368) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74375) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: nan, Accuracy: 27.5362\n",
      "Model saved to dmis_lab_biobert_v1.1_clinical_model.pt (Best validation accuracy: 27.5362)\n",
      "\n",
      "--- Finished training for dmis-lab/biobert-v1.1 ---\n",
      "\n",
      "--- Evaluating dmis-lab/biobert-v1.1 on the TEST SET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Evaluation:   8%|▊         | 1/13 [00:09<01:49,  9.11s/it]wandb-core(74391) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  15%|█▌        | 2/13 [00:16<01:28,  8.04s/it]wandb-core(74405) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  23%|██▎       | 3/13 [13:08<59:29, 356.96s/it]wandb-core(74414) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  46%|████▌     | 6/13 [53:17<1:46:07, 909.62s/it]wandb-core(74435) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  54%|█████▍    | 7/13 [53:24<1:01:27, 614.51s/it]wandb-core(74440) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  62%|██████▏   | 8/13 [53:47<35:30, 426.09s/it]  wandb-core(74442) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation:  77%|███████▋  | 10/13 [1:07:35<19:04, 381.57s/it]wandb-core(74453) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Test Evaluation: 100%|██████████| 13/13 [1:11:36<00:00, 330.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: nan, Test Accuracy: 0.2754\n",
      "\n",
      "--- Starting training for Simple RNN Model ---\n",
      "Custom tokenizer loaded. Vocabulary size: 15552\n",
      "Dataset split: Train=619 | Val=207 | Test=207\n",
      "RNNClassifierFromScratch initialized on mps.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>nan</td></tr><tr><td>val_accuracy</td><td>27.53623</td></tr><tr><td>val_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-water-21</strong> at: <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ng4nzcv4' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/ng4nzcv4</a><br> View project at: <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250803_193231-ng4nzcv4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb-core(74484) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74485) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(74499) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nishitha/Desktop/Learn/NLP/Clinical trials eligibility/PyTorch_Files/wandb/run-20250803_213536-7zr0prwe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/7zr0prwe' target=\"_blank\">stilted-brook-22</a></strong> to <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/7zr0prwe' target=\"_blank\">https://wandb.ai/dhopate-r-northeastern-university/NLP_Project_Clinical_Trials/runs/7zr0prwe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25 (Model: Simple RNN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 1/39 [00:11<06:58, 11.01s/it, loss=1.1]wandb-core(74515) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  10%|█         | 4/39 [05:46<1:17:20, 132.59s/it, loss=1.07]wandb-core(74578) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  21%|██        | 8/39 [06:04<14:56, 28.92s/it, loss=1.05]   wandb-core(74582) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  31%|███       | 12/39 [06:20<04:16,  9.51s/it, loss=1.06]wandb-core(74590) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  41%|████      | 16/39 [06:32<01:49,  4.78s/it, loss=1.07]wandb-core(74594) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Epoch 1:  49%|████▊     | 19/39 [06:49<07:11, 21.57s/it, loss=1.07]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 20\u001b[0m\n\u001b[1;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m train_and_evaluate_transformer_model(\n\u001b[1;32m      9\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name_to_test,\n\u001b[1;32m     10\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf_full\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m---> 20\u001b[0m rnn_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_rnn_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_LABELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN_RNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_RNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRNN_HIDDEN_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rnn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRNN_NUM_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRNN_DROPOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(rnn_result)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Comparative Study Summary ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 66\u001b[0m, in \u001b[0;36mtrain_and_evaluate_rnn_model\u001b[0;34m(df, num_labels, max_len, batch_size, epochs, learning_rate, embedding_dim, hidden_size, num_rnn_layers, dropout, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[72], line 26\u001b[0m, in \u001b[0;36mRNNClassifierFromScratch.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m lengths \u001b[38;5;241m=\u001b[39m lengths\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     22\u001b[0m packed_embedded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(\n\u001b[1;32m     23\u001b[0m     embedded, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m packed_output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_embedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m final_hidden_state \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]\n\u001b[1;32m     30\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_classifier(final_hidden_state)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Clinical_trials_eligibility-lIvJhYYY/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1136\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1126\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1148\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb-core(74605) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74639) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74650) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74676) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74707) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74711) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74730) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74749) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74796) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74845) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "wandb-core(74849) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_full = pd.read_csv('../Dataset/cleaned_data_3.csv')\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "\n",
    "    model_name_to_test = \"dmis-lab/biobert-v1.1\"\n",
    "    result = train_and_evaluate_transformer_model(\n",
    "        model_name=model_name_to_test,\n",
    "        df=df_full.copy(),\n",
    "        num_labels=NUM_LABELS,\n",
    "        max_len=MAX_LEN_TRANSFORMER,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_TRANSFORMER,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "    rnn_result = train_and_evaluate_rnn_model(\n",
    "        df=df_full.copy(),\n",
    "        num_labels=NUM_LABELS,\n",
    "        max_len=MAX_LEN_RNN,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_RNN,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_size=RNN_HIDDEN_SIZE,\n",
    "        num_rnn_layers=RNN_NUM_LAYERS,\n",
    "        dropout=RNN_DROPOUT,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_results.append(rnn_result)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Comparative Study Summary ---\")\n",
    "    for res in all_results:\n",
    "        print(f\"Model: {res['model_name']}\")\n",
    "        print(f\"  Best Val Accuracy: {res['best_val_accuracy']:.4f}\")\n",
    "        print(f\"  Final Test Accuracy: {res['final_test_accuracy']:.4f}\")\n",
    "        print(f\"  Final Test Loss: {res['final_test_loss']:.4f}\")\n",
    "        print(f\"  Saved Model: {res['saved_model_path']}\")\n",
    "        print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clinical_trials_eligibility-lIvJhYYY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
